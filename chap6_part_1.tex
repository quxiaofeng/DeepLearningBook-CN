\chapter{深度前馈网络}
\label{chap:6}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% author:jim1949@163.com %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% part:6.0-6.2           %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{深度前馈网络}又被称为\emph{前馈神经网络}或\emph{多层感知机}，是一种深度学习模型。前馈神经网络的目标是近似一个函数$f^*$。比如对于一个分类器来说，$y=f^*(\textbf{x})$ 把输入$\textbf{x}$映射到类别$y$。前馈网络定义了这个映射$\textbf{y}=f(\textbf{x};\bm{\theta})$，并且通过学习参数$\bm{\theta}$来近似真实的映射。


这类模型被称为\emph{前馈}是因为信息通过输入$\textbf{x}$流向用于定义函数$f$的中间计算，最后流向输出$\textbf{y}$。网络中并没有从输出流入模型的\emph{反馈}连接。当前馈网络扩展至包含反馈的连接时，就会被称为第\ref{chap:10}章中介绍的\emph{循环网络}。


前馈网络对机器学习的实践者非常重要。它组成了许多重要商业应用的基础模块。比如，用于照片中物体识别的卷积网络就是一种特化的前馈网络。前馈网络也是通往循环网络的重要踏脚石，而循环网络支撑了许多自然语言相关的应用。


前馈神经网络之所以被称为\emph{网络}是因为它非常典型的是由许多不同的函数组成的。这些函数的组成方式可以由一个有向无环图来描述。例如，函数$f(\textbf{x})=f^{(3)}(f^{(2)}(f^{(1)}(\textbf{x})))$可以由$f^{(1)}$、$f^{(2)}$、$f^{(3)}$链式的串联组成，而链式结构是神经网络中最常用的结构。在这个情况中，$f^{(1)}$被称为网络的\emph{第一层}，$f^{(2)}$被称为网络的\emph{第二层}，以此类推。这个链的长度就定义了网络的深度。前馈网络的最后的一层被称为\emph{输出层}。在神经网络的训练过程中，我们不断使得$f(\textbf{x})$近似于$f^*(\textbf{x})$。在不同的训练节点上，训练数据提供了关于$f^*(\bm{x})$的带有噪声、近似的样本。每个样本$\bm{x}$都有一个使得$y \approx f^*(\bm{x})$的标签。训练数据指定了在每个$\bm{x}$上输出层的直接表现，但没有直接指定其他层的行为。学习算法需要决定如何使用这些层来产生相应的输出，但训练数据并未指定每一层需要做什么。所以学习算法需要知道如何使用这些层来对$f^*$进行最优的近似，因为训练数据并不会为每一层都指定输出，这些没被指定输出的层就被称为\emph{隐层}。


最终，这些网络被称为\emph{神经}，因为它们是由神经科学启发而来的。网络的每个隐层都是典型的向量值，隐层的维度决定了模型的\textbf{宽度}。这些向量中的每一个值都可以被看做对一个神经元的模拟。与其把这些层当做是向量间的映射函数，不如把每一层当做由许多并行的\emph{单元}组成，而每个单元代表着一个向量到标量的映射函数。每个单元在这个场景下都被当做一个神经元，它从其他的单元中获取输入并计算自己的激活值。这种使用向量来代表网络层的想法来自于神经科学。$f^{(i)}(\bm{x})$的计算行为也是或多或少来自于对生物神经元计算行为的观察。然而，现代的神经网络研究也
受了许多数学和工程的指导，而且其目的也并非是完美的模拟大脑。我们最好把前馈网络当初一种函数近似技术，用于达到统计上的概括，偶尔会借鉴一些我们对大脑的先验知识，而非用于对大脑进行建模。



对前馈网络进行了解的一种方法是先从线性模型开始，并考虑如何突破线性模型的限制。如逻辑回归和线性回归等线性模型非常吸引人，因为他们是封闭形式的或可以进行凸优化，可以非常有效和可靠的进行拟合。但线性模型也有个显著的缺点，它们只能拟合线性函数，因此线性模型不能理解两个输入变量之间的相互作用。


为了使得线性模型可以表达$\bm{x}$的非线性函数，我们可以把输入转换为$\phi(\bm{x})$，$\phi$是一个非线性转换。等效的，我们也可以用\ref{sec:5.7.2}章中提到的核方法，通过应用$\phi$的映射来获得非线性的学习算法。我们可以认为$\phi$提供了一组可以描述$\bm{x}$的特征，或认为提供了$\bm{x}$的新的形式。


问题在于，我们如何选择映射$\phi$。

\begin{enumerate}
 	\item 一个选择是使用通用的$\phi$， 比如RBF核用的无限维的$\phi$。如果$\phi(\bm{x})$的维度足够高，总能有办法来拟合训练集，但测试集上的泛化缺是个问题。通用化的特征映射通常只基于局部平滑的原则，并没有编码进足够的先验信息来进一步解决问题。
	\item 另一个选择是手工的$\phi$。在深度学习来临之前，这是一个主流方案。这个方法对每个不同任务都耗费数十年的人力，并需要实践者在不同的领域如语音识别或计算机视觉进行特化，且领域之间的共同点非常少。
	\item 深度学习的策略则是学习$\phi$。在这个方法中，模型是$y=f(\bm{x};\bm{\theta},\bm{w}) = \phi(\bm{x};\bm{\theta})^T\bm{w}$。参数$\theta$可以用于从许多的函数中学习到想要的$\phi$，参数$\bm{w}$用于控制$\phi(\bm{x})$到指定的输出。这是一个深度前馈网络的示例，$\phi$是其隐层。这个方法是三个中唯一放弃了训练问题的凸性质的，但其优点远胜于缺点。在这个方法把输入转换后的形式记为$\phi(\bm{x};\bm{\theta})$，使用优化算法来找到使得转换后的性质最优的$\theta$。通过提供一个非常广阔的$\phi(\bm{x};\bm{\theta})$族，我们可以让这个方法像第一个方法一样具有高通用性。它也具有第二个选择的优势。人可以通过设计$\phi(\bm{x};\bm{\theta})$族来编码自身的知识，并帮助泛化。其优势是人类设计者只需要找到正确的函数簇而非找到唯一的正确的函数。
\end{enumerate}


通过学习特征来提高模型性质的准则超出了本章前馈网络的讨论范围。但这是深度学习的主题，可以适用于本书描述的任何模型。前馈网络的原则是学习从$\bm{x}$到$\bm{y}$的确定性映射，并且不引入反馈连接。后面介绍的一些模型会使用这些原则来学习统计性映射，学习带反馈的函数，或学习在一个向量上的概率分布。


我们以一个简单的反馈网络示例开始本章节。然后，我们解决部署前馈网络所需的设计决策。首先，训练前馈网络需要很多和设计线性模型一样的设计决策：选择优化器、损失函数、输出单元的形式。我们先回顾基于梯度学习的基本知识，然后再看一些前馈网络独有的设计决策。