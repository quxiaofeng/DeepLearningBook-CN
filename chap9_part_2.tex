%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% author: iWeisskohl %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% part:9.7-9.11      %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{结构化输出}
\label{sec:9.6}


    卷积神经网络不仅可以预测一个分类任务的类别标签或者一个回归任务的真实值，还可以输出一个高维度，结构化的目标。通常该目标是由标准化的卷机神经网络输出的一个张量。例如，该模型输出一个张量S,S(i，j,k)是输入网络的像素P(j,k)属于第i层网络的概率。这允许该模型给图像中的每一个像素做标签，同时在单个物体的外围绘制精确的掩膜。

如图9.13所示，我们经常遇到的一个问题是输出平面比输入平面小。在各种被用于区分一副图像中的单个物体的网络结构中，网络空间维度最大的损失来自于使用池化层的大步长。为了产生一个和输入图像相同尺寸的输出图，我们可以尽量避免同时池化（Jain et al,2007）。另一个策略是简单的shu一个较低分辨率（ ）的网格标签。最后，从理论上来说，我们可以对一个步长的像素进行池化操作。

对图像进行像素级别标签首先需要对图像的标签产生一个初始的预测，然后通过在相邻像素间的交互影响调整之前的预测。重复这个步骤相当于在每一阶段用同样的卷积层，在深度网络的最后一层之间共享权值。连续卷积层通过循环神经网络共享层间权值共享来进行一连串计算。图9.17展示了循环神经网络的结构。

一旦完成对某个像素的预测，为了把图像分割成不同的区域，我们有很多种办法可以来进一步处理这些预测的像素点（）。大概的思路是一个区域内的连续像素往往属于同于标签的。图像模型可以描述相邻像素间的相关性。另外，卷积神经网络可以被训练来最大化一个近似的图形化模型训练目标。

\section{数据类型}
\label{sec:9.7}

      被用于卷积神经网络的数据通常由几个通道组成，每个通道在空间和时间的某些节点观察一些特征信息。表格9.1展示了不同维度和数量的通道的数据。

我们可以参考卷积神经网络的视屏（），上面提到了相应的例子。
  
  
目前为止我们仅仅讨论了训练和测试数据是同样空间维度的情况。卷积神经网络的一个优点是可以处理不同空间范围的输入。这种类型的输入不能被传统的，基于矩阵相乘的神经网络替代。这向我们展示了卷积神经网络的迷人之处，尽管网络的计算开销和过拟合存在很大影响。
 
 
例如，现考虑一系列的图片，每一张图片都有不同的长度和宽度，我们很难为这些图片找到一个固定尺寸和权值的矩阵模型。卷积很明显是适用的，卷积核随着输入图片的尺寸调整不同的参数，同时卷积操作的输出尺度也相应的做改变的。卷积可以看成矩阵相乘，相同的卷积核为不同尺寸的输入图像诱导不同尺寸的双块循环矩阵。有时候神经网络允许输出和输入一样有不同的尺寸，例如如果我们想要把一个类别标签分配给每一个输入像素。在这种情况下，我们不需要做其他事情，换句话说，网络本身必须生成合适尺寸的输出，例如我们想要在一整副图像中分配单类别标签。在这种情况下我们必须要做一些另外的步骤设计，比如插入共用池化层，它的池化区域尺度在尺寸上与输入图片的成比例，为了获得一个固定的池化输出。图9.11列出了一些例子。

注意到把卷积层用到处理不同尺寸的输入只有在输入图片是不同尺寸时才有意义，因为它们包含对同一类物体的不同观察——随着时间推移不同长度的记录，随着空间推移不同深度的观察等等。如果输入有很多种尺寸，那么卷积并没有意义，因为它可以随意的包含不同种类的观测信息。例如，我们在处理一个大学的申请，我们的特征包括年纪和标准化的测试成绩，但是并不是每个人都参与了这个测试，所以和所有的特征（类似于此处的年纪和测试成绩）使用相同的权值去卷积是没有意义的。


\section{有效的卷积算法}
\label{sec:9.8}
 传统的卷积神经网络应用往往包含至少100万个神经元，如12.2中提到的，利用强大的并行计算资源是必不可少的。然而，在某些情况下我们也可以通过选择合适的卷积算法来提高卷积运算的速度。

   卷积相当于用傅里叶变换把输入和卷积核卷积操作到频率域，对两个信号用点乘，再用傅里叶反变换变换到时域。在某些情况下，这可能比用离散卷积计算更快。

   当k维卷积核可以被表示为d维向量的外积，每一维一个向量，那我们说这个卷积核是可分离的。当卷积核是可分离的，那朴素卷积就失效了。它等价于用每个一维向量组成d个一维向量。这种组成方法很明显比对d个一维向量进行外积要快。这个卷积核同样比向量需要较少的参数。如果卷积核每一维有w个元素，那么朴素的多维卷积需要 O(wd) 的运行时间和元素存储空间，然而可分离卷积只需要 O(w*d)的运行时间和存储空间。当然，并不是每一个卷积网络都可以这样表示。
   
   在不损害模型精度的前提下设计出执行卷积或者近似卷积的更快的方法是一个很有意义的研究领域。尽管在商业领域我们采用一定的技术已经提高了前向传播的效率，我们更需要将更多的精力和资源用于网络的开发而不是训练。
   
   
   
\section{随机或无监督特征}
\label{sec:9.9}

很明显的，卷积过程中开销最大的部分是特征学习部分。输出层的开销相比于特征学习这部分来说是很小的，因为经过多个卷积层的池化之后输入到该网络层的特征数量是很少的。当我们执行基于梯度下降的监督学习时，每一步的梯度下降需要在整个网络中执行一次完全的前向传播和反向传播。减少卷积网络的训练开销的一个方法是使用监督领域未被训练的特征。

    这里有三种基本的策略可以在没有监督训练的前提下获得卷积核。一个是任意的初始化卷积核。一个是手工设计卷积核，例如设计每一个卷积核检测某一个特定方向或者尺度的边缘。最后，我们可以用过无监督准则习得卷积核。例如，（coates et al(2011)）对区域图像shi用K均值聚类的方法，然后把学习到的矩心作为卷积核。第Ⅲ部分描述了很多非监督学习的方法。在无监督准则下学习特征允许我们在顶层的分类层里把他们区分开来。然后我们就可以在整个网络集里一次提取出全部特征，简单的构造一个最后一层网络的新的训练集。学习最后的卷积层是一个很典型的卷积操作问题，我们假设最后一层是类似于逻辑回归或者SVM分类器。
    
    随机滤波在卷积网络中往往有很好的效果（），saxe et al.向我们展示了当我们随机分配权值的时候由自然池化组成的卷积层具有频率选择性和平移不变性。他们声称这提供了一种代价较小的选择网络的办法：首先通过仅训练最后一个卷积层来评估几个卷积网络的构造，然后选取其中表现最好的网络结构并用一种开销较大的方法训练整个网络。
    
    一个折中的方法是学习特征，但是用这种方法并不需要在每一梯度计算时都进行全部的前向和反向传播。与多层感知器一样，我们采用逐层预训练的方法，独立的训练第一层，然后仅从第一层中提取所有的特征，然后基于已有的特征独立的训练第二层网络，等等。第八章描述了怎么样去执行有监督的逐层预训练，第三部分对每一个卷积层用无监督原则拓展了逐层预训练。一个比较权威层级预训练卷积模型是深度卷积神经置信网络（）。卷积神经网络给我们提供了一个比多层感知器更进一步的预训练策略。相比于之前一次训练整个神经网络，我们可以训练一个小块的模型，像Coates et al.(2011)在k均值聚类中做的一样。然后我们可以用从基于块模型的参数来定义卷积层的卷积核。这也意味着我们可以在不用卷积训练的情况下用无监督学习来训练卷积神经网络。采用这种方法，我们可以训练很大网络，同时只在推理阶段会产生很大的计算开销。（ Ranzato et al. 2007b； Jarrett et al. 2009； Kavukcuoglu 2010 Coates 2013 ;Coates et al,2013）这种方法在2007-2013年间是很流行的，当时的标签数据集是很小的而且计算能力也有限。今天，大部分的卷积神经网络趋向于采用完全的监督学习，在每次迭代过程中用完全前向传播和反向传播。
 
   至于用其他方法来进行无监督训练，尽管我们能从中看到一些益处，但仍旧值得进一步研究。无监督学习提供了我们一些和监督学习之间的正则化的关系，又或者因为学习规则的变化减少了计算开销，我们可以训练更大的网络结构了。
   
   





