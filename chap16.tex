\chapter{深度学习的结构化概率模型}
\label{chap:16}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% author:YisenWang %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
深度学习借鉴了很多模型形式方面的内容来帮助研究者指导他们的设计理论和算法描述。这其中之一就是有结构的概率模型。我们在前面3.14节的时候，简单的讨论过一些有结构的概率模型。那些简短的介绍已经足够用来理解怎么使用有结构的概率模型来描述第二部分的一些算法。在第三部分，有结构的概率模型是深度学习里很多重要的研究方向的关键成分。为了后续讨论这些研究点，在这一章，我们会详细介绍有结构的概率模型。不过，读者不用担心，这一章是自我完备的，在开始学习这一章之前，读者不需要复习之前的介绍。

有结构的概率模型是描述概率分布的一种方式，它直接通过图的形式来描述概率分布中随机变量之间的关系。这里，我们沿用了图理论里面图的概念，即，节点之间通过边来连接。因为模型的结构是由图来定义的，因此，这些模型也经常被称作图模型。

图模型的研究领域很大，也已经发展了很多不同的模型、训练算法和推断算法。在这一章，我们主要讲一些图模型里最核心的思想，并把重点放在那些对深度学习的领域很有用的一些概念上。如果你已经有很强的图模型背景，你可以跳过本章的大部分内容。但是，即使是一个图模型方面的专家，他也可能会从这一章的最后一部分（16.7节）收益，因为，在16.7节，我们会高亮一些特有的图模型可以用在深度学习里的方法。相比于图模型的研究者，深度学习的参与者更倾向于用非常不同的模型结构、学习算法和推断过程。在这一章，我们指出了他们在偏好上的不同，并解释其中的原因。

在这一章，我们首先描述了建立大规模概率模型的挑战。接着，我们描述了怎么用图来描述一个概率分布的结构。虽然这种方法允许我们克服许多挑战，但它不是没有自己的复杂性。在图模型里，一个最主要的困难是理解在一个图里，哪些变量之间需要直接相连，也就是，对于一个给定的问题，哪种图结构是最合适的。我们在16.5节简介了两种方式来解决这个问题。最后，我们通过讨论深度学习与图模型之间的关系来结束本章。

\section{无结构模型的挑战}
深度学习的目标是延伸机器学习到解决人工智能所面临的各种挑战，这意味着深度学习能够理解具体很丰富结构的高维数据。比如说，我们希望人工智能算法能够理解自然图像，声波表示的语音，以及包含多个单词和标点符号的文档。

分类算法能够从很高维的分布中取出一个输入并用一个类别标签总结他，比如照片里面是个什么物体，语音说的是哪个词，文档是关于哪个话题的。分类的过程忽略了输入里的大部分信息，只产生了一个输出（或那个单一输出值的概率分布）。一个分类器也经常忽略输入的很多部分。比如，当识别图片中一个物体，经常会忽略图片中的背景。

让概率模型做很多其他的任务也是可能的。这些任务通常比分类更昂贵，比如，其中一些可能需要输出多个值，而且大部分要求一个对输入有一个完整的理解，不能忽略其中的任何一部分。这些任务包括：
\begin{itemize}
\item 密度估计：给定一个输入$x$, 机器学习系统返回一个在数据生成分布下的真实密度$p(x)$的估计。这虽然只要求一个输出，但它需要对整个输入有一个完整的理解。即使是输入向量中只有一个元素是不寻常的，该系统也必须给它分配一个很低的概率。
\item 去噪：给定一个损坏的或不正确的观测输入$\hat{x}$，机器学习系统返回一个原始或正确$x$的估计。比如，机器学习系统可能被要求从旧照片去除灰尘或划痕。这需要多个输出（估计的干净样本的每个元素）和对整个输入的理解（因为一个损坏的区域将仍然导致最终的估计是损坏的）。
\item 缺失值的插补：给定$x$的一些元素的观察，要求模型返回在一些或所有未观察到的$x$元素的估计或概率分布。这需要多个输出。因为模型可以被要求恢复$x$的任何元素，它必须理解整个输入
\item 采样：该模型从分布$p(x)$生成新样本。应用包括语音合成，即产生像自然人类语音一样的新波形。这需要多个输出值和整个输入的模型。如果样本甚至有一个元素来自错误的分布，那么抽样过程是错误的。
\end{itemize}

比如一个用很多小的自然图像采样的一个例子，如图16.1所示。

在数千或数百万的随机变量上建模丰富的分布是计算和统计学上的挑战性任务。假设我们只想模拟二进制变量。这是最简单的可能情况，但已经是压倒性的。对于小的$32×32$像素的彩色（RGB）图像，存在$2^3072$个这种形式的可能的二进制图像。这个数字比宇宙中估计的原子数多$10^800$多倍。

一般来说，如果我们希望对一个包含$n$个离散变量且每个变量能取$k$个值的随机向量$x$建模分布，则通过存储具有每个可能结果的一个概率值的查找表来表示$P(x)$的朴素方法需要$k^n$参数！

由以下几个原因导致这样做不可行：
\begin{itemize}
\item 内存：存储表示的成本：对于除了非常小的$n$和$k$之外的所有值，作为表示分布的表将需要太多的值来存储。
\item 统计效率：随着模型中参数数量的增加，使用统计估计器来选择参数值所需的训练数据量也增加。因为基于表的模型具有天文数量的参数，所以需要天文大的训练集来精确拟合。任何这样的模型将在训练集上有非常严重地过拟合，除非作出连接表中差异的附加假设（例如，像在back-off或平滑的n-gram模型中，第12.4.1节）
\item 运行时间：推断成本：假设我们要执行一个推断任务，我们使用联合分布$P(x)$来计算一些其他分布，例如边际分布$P(x_1)$或条件分布$P(X_2|x_1)$。计算这些分布将需要对整个表进行求和，因此这些操作的运行时间与存储模型的不可计算存储成本一样高。
\item 运行时：抽样成本：同样，假设我们要从模型中抽取一个样本。 这样做的天真的方法是抽样一些值$u〜U（0,1$，然后通过表迭代，添加概率值，直到它们超过并返回对应于表中该位置的结果。这需要读取整个表 在最坏的情况下，所以它具有与其他操作相同的指数成本。
\end{itemize}

基于表的方法的问题是我们明确地建模每个可能的变量子集之间可能的交互。 我们在实际任务中遇到的概率分布比这更简单。通常，大多数变量仅间接地相互影响。




